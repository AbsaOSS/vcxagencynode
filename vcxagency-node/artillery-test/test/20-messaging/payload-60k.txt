-- todo
- skt-hlf: hlf-config.yaml에 document ver 필드 추가
  - load 시 낮은 버전 문서를 현재 버전으로 converting하는 기능도 필요
- skt-hlf: Orderer 파라메터 (블록 타임아웃 등) 설정 방법 제공
  . cryptogen, configtxgen을 구동하는 스크립트 파일을 생성하는 것이 좋겠다
    generate 명령후, 스크립트 파일을 구동하면서 미세조정을 할 수 있게...
  . hlf-master.sh 제공?
- skt-hlf: dns server 설치/운영
  . 당위성: 컨테이너 extra_hosts 등록 방식 (즉, 컨테이너의 /etc/hosts 이용 방식)으로는
    peer, orderer의 주소가 바뀔 경우 대응하지 못함
  . 당위성: HLF 네트워크 구축 시작 단계에서 모든 서버(그리고 IP)가 준비되지 않음
    추후 점진적으로 서버(그리고 IP)를 추가/변경할 때마다 DNS 설정이 동적으로 일어나야 함
  . 요구사항: DNS 서버는 Org마다 별도 운영할 수 있어야 함
    DID 하나은행에서 proxy를 이용했을 때, Org마다 proxy 주소를 달리설정해 주어야 했음
  . 요구사항: Org별 DNS서버에 항목을 추가/수정/삭제하는 기능을 제공해야 한다
- skt-hlf: "cc deploy전 channel ready 상태"를 event로 처리
- skt-hlf: Raft Orderer 검토
  - Kafka Orderer 마이그레이션 기능 필요
- skt-hlf: hlf-config.yaml cc 파트에 invoke/query-args 필드 추가, invoke/query-cc.sh 자동 생성
- 브랜드사이트 론칭
  - 브랜드사이트 수정안 정리 & 협의 (w/ 주재현 님, 이윤지 님)
    - 오픈 범위 협의 (DApp 샘플, 프로비저닝웹, ...)
      - DApp 샘플 다듬기 (필요?): 성택님 DApp? MWC DApp?
    - 도메인 협의 (stonledger.io?)
  - 프로비저닝웹 다듬기 (필요?)
    - cc path 속성을 read-only로 고정
    - query-cc, invoke-cc 포함?
    - README 리뷰
  - https 인증서 확보
- Azure VM Image 작성 방법 파악
- Azure향 BaaS
  - Azure 메타트론 디스커버리 사례 확인
    - 페이지내 정보
      - 동영상 필요한가?
    - 아키텍처 다이어그램
- skt-hlf: grafana monitoring refactoring
  - hlfConfig Monitor 아이템의 타입 체크 (XXXSpec은 단일 아이템, XXXs는 array인 규칙 고려)
  - host-containers map 상에서 monitor 항목 하나로 여러개의 container들을 처리하는 것 개선
  - (grafana와 같은 머신에서 실행되는) prometheus의 포트는 host에 노출될 필요 없을 것 같다
  - prometheus 등의 monitoring container들에 extra_hosts 설정이 필요 없을 것 같다
- hlf-skt: long-term test
  !- orderer load balancing? : 왜 o1만 일하나? (o1만 작업로그가 출력됨)
    => invoke-cc에 o1만 명시했다
    !- invoke-cc를 수정하면 모든 orderer들을 참가시킬 수 있나? => yes
      - invoke 성공하지만, query하면 결과가 그대로다 (invoke한 값이 반영되지 않았다). 원인?
  - grafana vm 상태 파악에 누락되는 node들이 있다 (cadvisor? node-exporter?)
  - peer/orderer 컨테이너 교체 프로세스 테스트
    . HLF 1.4.7 안정화 버전
    . volume mount 옵션 적용 버전
    - nCloud 서버의 스토리지는 컨테이너 볼륨 전체를 백업할만큼 넉넉한가?
- initial-ledger: org 추가
  - n-cloud 계정/로그인 확인
  - n-cloud 서버 접속 확인
  - 방화벽 확인
  - 프락시 확인
  - HLF org/peer 추가
- hlf-longterm: disk full 문제 해결
  - 1일 tx 조절 (orderer 머신 스토리지에 3+개월 저장 가능하도록 tx 낮추기)
  - grafana page에서 일부 vm이 보이지 않는 문제 fix
  - grafana 초기화 스크립트 검증 및 fix
- did front-end

- Mediator 기능 테스트 자동화
  - Faber-Alice jest 테스트 스크립트 작성
  - Q to Author
    - vcxagency-node에서 yarn run test:unit 하면 실패, 의도적인가?
      vcxagency-node/src/configuration/app-config.js 파일 64 라인의 아래 구문 (min 0, max 0)
      PG_WALLET_MIN_IDLE_COUNT: Joi.number().integer().min(0).max(0)
- Mediator infra 구축 (for 운영)
  - postgres HA 운영 Best practice 파악
- Mediator docker image 빌드
  - 환경변수 및 설정 파라메터 선택
    . 로그출력 없게/적게
    . DB connection pool 이용을 효과적으로
- Mediator 성능 테스트
  !- 테스트 도구 탐색: Artillery 적합성 테스트 (JS function 만으로 시나리오 구성이 가능한가?)
    => NodeJS 부하 테스트 도구로는 Artillery 외에 대안이 없는 것 같다
    => 그런데, Artillery는 JS function 만으로 테스트 시나리오를 구성할 수 없다 (api-url & payload 명시 필요)
    => 그러니, vcxagency-node 대상 api-url과 payload를 사전에 구비할 수 있는지 확인하자 (AbsaOSS 문서 정독)
  !- 평가 지표 & 평가 시나리오 개발
    . PASS앱의 경우 최대 동접 2천명 (@ 7.17타운홀 자료) => 최대 동접 2,000 명이 목표
    . (서버 1대에서 동작하는) Mediator 1개 인스턴스의 처리 용량 산정
      . (4CPU,8G RAM,... 스펙의 머신은) 1초에 몇개의 VC발급이 가능한가 (또는 Proof수집이 가능한가?)
    !- 인터넷 자료 정독
    !- Poll-me 자료 정독
    => proof-req payload size, proof payload size 요청함 (to 지혁님)
    => proof 확인 시나리오로 1개 Agency의 최대 퍼포먼스를 측정하기로 함
  - 테스트 인프라 구축 (long-term 서버 활용)
    . VM list
      . 172.27.19.109: operating vm (4core cpu, 7.8G mem)
      . 172.27.19.83: Agency (4core cpu, 7.8G mem)
      . 172.27.19.55: postgres db (4core cpu, 7.8G mem)
      . 172.27.19.32: indy-pool (4core cpu, 7.8G mem)
      . 172.27.26.31, 104, 33, 30, 80, 96: load gen (4core cpu, 7.8G mem)
    !- initial-mediator ingee/for-ltvm branch 생성
    !- SW 설치 (19.109, 19.83, 19.55,
      !- vim
      !- node.js: nvm install/use/alias lts/erbium
      !- initial-mediator clone & checkout ingee/for-ltvm
    !- SW 설정/실행
      !- loader들 docker.txn 확인 => ledger 쓰지 않음 (schema, schema-def 정의 않음)
    !- Agency version에 따른 성능차? => 최신 버전으로만 테스트!
    - test summary : 테스트 환경 구축
      !- max-vuser 제한을 없앰
        . msg-0728-1525.log
        . ld별 Faber들의 send msg는 14000 여개, Alice들의 recv msg는 250 여개
          => ECONNRESET (1090개), ESOCKETTIMEOUT (10900개) 에러로 msg 전달 실패함
      !- Agency log level을 warning으로 높임 (더 적게 출력)
        . msg-0728-1546.log
        . total-msg-count, faber-msg, alice-msg 수치 비슷
        . ECONNRESET=1090개, ESOCKETTIMEDOUT=10900개 (직전 1090개, 10900개)
        . median res-time = 26 sec (직전 25 sec)
      !- Agency log level을 error으로 높임 (가장 적게 출력)
        . msg-0728-1634.log
        . total-msg-count, faber-msg, alice-msg 수치 비슷
        . ECONNRESET=1090개, ESOCKETTIMEDOUT=10900개 (직전 1090개, 10900개)
        . median res-time = 28 sec (직전 26 sec)
      !- Artillery instance를 2배로 (2 proc/vm)
        . msg-0728-1718.log,
          msg-0728-1728.log: artillry fault 발생, 비정상 종료로 지표 확인 못함
        !- test code fix ==> 포기. 이런 상황이 발생하지 않도록 제한하겠다 (vCPU 점유율 100% 이내로)
    !- 중간 정리
      !- libindy 관련 patrik 메시지 확인 (blocking io?) => wallet API가 blocking 방식으로 동작한다
        ---
        @VCX Patrik Stas 5:14 PM
        Yeah also the agency - on the node implementation I've hit bottleneck of roughly 70 request/second == 4200 a minute. I've seen benchmark where NodeJS Express server handled about 20K/second. From my experiments, I've managed to get about 900/second (maybe I did something wrong) - but my conclusion is that if the wallet API is async, 1 agency instance will handle I think at least 1K/second, if not more.
        Till then the workaround I see is to run many container instance using some docker orchestration tool
        ---
      !- Artillery socket-timeout 설정 가능한가? => config/http/timeout 에 초단위로 설정 (default 120초)
        . https://artillery.io/docs/http-reference/#request-timeout
        ---
        config:
          target: "http://my.app"
					http:
						# Responses have to be sent within 10 seconds or the request will be aborted
						timeout: 10
        ---
    !- test summary: Agency CPU 100% 부하는 어느 정도 일까? (그러면 total-msg == sum-of-Alice 가 될까?)
			. Agency 로그를 근거로 timeout을 300초로 설정
				. msg-0729-1246.log
				. ECONNRESET(12000)개 발생, ESOCKETTIMEDOUT은 발생 않음 => Artillery는 끊지 않았지만, 서버가 끊었다
        . Agency res time max= 280sec
        . Agency %cpu max= 200%
      . config.http.timeout 원복, loader 6개->4개로 축소
        . msg-0729-1301.log
        . ECONNRESET은 9000개씩, ESOCKETTIMEDOUT은 800개, 100개, ...
        . Agency crash!
        . Agency res-time-max= 125sec
        . Agency %cpu-max= 180%
      . loader 4개->2개로 축소
        . msg-0729-1307.log
        . ECONNRESET은 8000개씩, ESOCKETTIMEDOUT은 발생 않음, ECONNREFUSED 발견됨
        . Agency crash!
        . Agency res-time-max= 41sec
        . Agency %cpu-max= 150%
      . loader 2개->1개로 축소
        . msg-0729-1311.log
        . ECONNRESET은 10개 미만씩, ESOCKETTIMEDOUT은 7분 경과후부터 1000개씩 발생
        . Agency alive!
        . Agency res-time-max= 253sec
        . Agency %cpu-max= 통상 150%, 190%도 잠깐씩 지나감
        => loader 1개로 Agency를 죽일 수 있다 (더이상의 loader 투입은 필요 없다)
      . loader 1개, maxVusers 100
        . msg-0729-1335.log, msg-0729-1409.log
        . ECONNRESET, ESOCKETTIMEDOUT 미발생
        . Agency alive!
        . Agency res-time-max= 0.8sec
        . Agency %cpu-max= 150%
        => total-msg == total-alice!
      . loader 1개, maxVusers 200
        . msg-0729-1418.log
        . ECONNRESET, ESOCKETTIMEDOUT 미발생 (total-msg == send-msg == recv-msg)
        . Agency alive! %cpu-max= 150%
        . Agency res-time-max= 3.5sec
        . res-time-max= 3.9sec, res-median=1.0sec
      . loader 1개, maxVusers 400
        . msg-0729-1426.log
        . ECONNRESET, ESOCKETTIMEDOUT 미발생 (total-msg == send-msg == recv-msg)
        . Artillery hang!
        . Agency alive! %cpu-max= 150%
        . Agency res-time-max= 11.3sec
        . res-time-max= 10.5sec, res-median=9.4sec
      => loader 1개, phase 20/120, arrivalRate 2/100, maxVusers 100 선택
        (이 상태에서 total-msg == total-Alice 였다)
    !- test summary: Agency %CPU < 100 적정 부하패턴 검색
      => loader 3개, phase 10/120, arrivalRate 1/4, maxVuser 4 선택
        (이 상태에서 Agency %CPU가 80~120 이었다)
    !- test 목표치 설정
      . max %CPU, 80%(top 출력치)를 목표치로 => 이를 넘으면 NodeJS 로직이 GC와 경쟁
        => https://artillery.io/docs/faq/#high-cpu-warnings 참조
      . max response-time, 1sec를 목표치로 => 그냥... %CPU에 종속 (무시해도 무방)
    !- test summary: response time, res/sec 의미 파악 (post 행위에 대한 지표가 맞는가?)
      !- 비교 기준으로 활용하기 위해 5번 결과치 산출 => res/sec=112.06
      !- loop 다음의 summary 호출을 제거하고 지표가 어떻게 변하는지 확인 (추정: 동일) => res/sec=112.11, 부합
      !- loop 횟수를 2로 줄이고 지표가 어떻게 변하는지 확인 (추정: 1/5배) => res/sec=23.21, 부합
      !- loop 횟수를 20으로 늘리고 지표가 어떻게 변하는지 확인 (추정: 2배) => res/sec=145.96, 성능Max도달/포화
      => Alice가 Agency에서 msg를 가져올 때 생기는 부하가 무척 크다 (안 가져올 때 %CPU=50, 가져올 때 %CPU=130)
    !- test summary: Agency 한계치 측정 (Agency의 최대 TPS는 60인 것 같다, jjh 추정)
      . 테스트결과: https://docs.google.com/spreadsheets/d/1E8GrWpDAxOzirsTo0PV9qvSYNpTqAZ8tuGPG_SyJckY
      . CPU max 100% (top 출력치) 이내를 목표치로 부하 제한 => Agency가 정상 응답하는 최대치라고 생각
      => Agency TPS (== res/sec로 추정)은 100
    - test summary: Agency post call의 payload 크기를 키워보자 (res/sec가 변하는가?)
      . yjh님 log에 의하면 가장 큰 post payload는 verification 때 60477[byte] => 134k text file 확보/이용
    - Agency 튜닝 factor 확인
      - log level?
      - PG_WALLET_MIN_IDLE_COUNT?
    - share w/ team
      . max %CPU, 80%(top 출력치)를 목표치로 => 이를 넘으면 NodeJS 로직이 GC와 경쟁
        => https://artillery.io/docs/faq/#high-cpu-warnings 참조
        => loader 3ea 사용
      . 지표 취합 방식
        . response time, max : 3ea 중 최대치
        . response time, med : 3ea 중 최대치
        . res/sec : 3ea 합산
      . Alice가 Agency에서 msg를 가져올 때 생기는 부하가 무척 크다 (안 가져올 때 %CPU=50, 가져올 때 %CPU=130)
    - git commit (agency 소스&설정 initial-mediator로 가져오기)
      - test log @ld1
      - test setting, src @ld1
    - perf-test branch cherry-pick
      - 5fa11f865452ceb507c0eaa5f7e9d0627bed93a7 : perf-test/README.md
      - 65eff790d6627ce11f08cfe8cfa7afa33c54caa9 : .../20-messaging/artillery.proc.js : Alice-total
      - 7423cac1e7370b7b6cfb594b015e72e37ea0ad8e : payload file for post op

-- weekly

-- done
0723- Mediator 성능 테스트 코드 개발: faber(N개) vs alice(N개) messaging
  . nmon 설치 (시스템 메트릭 모니터링 툴)
  !- Config.json의 DEBUG 파람과 log 함수 연계
  !- vcxagency-client 분석, 테스트 설계: 1:1? 1:N? N:N?
    => "faber send msg to alice", "alice send msg to faber" 시나리오 별도 구비
    => "faber download msg from agency", "alice download msg from agency" 시나리오도 구비
  !- artillery 테스트 코드 구현
    !- agentConn 1개로 여러개의 entity들로부터 msg를 받을 수 있는가? => 있다
    !- 1:1 messaginge 테스트 코드를 N:N messaging 으로 일반화
      !- post-req:: loop 처리
      !- alice max count 처리
        => alice 수가 많아지면 messaging 부하 테스트가 아니라 onboarding 부하 테스트가 될 우려가 있다
        !- test-config.json 파라메터 추가
      !- 시나리오 끝날 때, alice 가 수신한 메시지 숫자 카운트
    !- anoncrypt wallet name 고정
  !- wrap-up
    !- 10-onboarding, AgencyDid/Verkey를 test-config.json에서 읽도록 artillery.proc.js 수정
    !- artillery.yaml:: test-phase 적절하게 디폴트 설정
  !- README.md 업데이트
    !- 테스트 설정 방법
      . agency url : artillery.yaml
      . agency did, verkey : test-config.json
      . Alice/Faber number : test-config.json
      . MaxMessaging Alice/Faber number : test-config.json
  !- git commit: check to remove a.log
  !- test env set-up: w/ P.CL VMs
  !- Share with team
    . 데모
    . test metric 의미 (total msg count, faber msg count, alice msg count)
    . test-config.json Faber/Alice MaxNumber 의미
      => alice 수가 많아지면 messaging 부하 테스트가 아니라 onboarding 부하 테스트가 될까 우려
    - Q: 어디까지가 표준인가? vcxFlowFullOnboarding, vcxFlowGetMsgsFromAgent, ... 는 표준인가?
0715- Mediator 성능 테스트 코드 개발: faber(N개), alice(N개) onboarding
```
nc -l 8080
curl -v -X POST localhost:8080/agency/msg -d @vcx-connect-alice.msg -H "Content-Type: application/ssi-agent-wire"
curl -v -X POST localhost:8080/agency/msg -d @vcx-connect-alice-dbg.msg -H "Content-Type: application/ssi-agent-wire"
DEBUG=[*|http|http,http:response] artillery run TEST_YAML
```
  !- vcx connect
    !- Artillery로 vcx-connect 부하테스트 코드 작성
    !- Artillery README 작성
    - vcx connect 메시지를 file에서 읽어오는 방식이 아니라 테스트할 때 동적으로 생성하도록 수정
      => test:10 vcx-msg-conn 캡처 과정이 필요 없어짐
      !- init의 indyCreateAndStoreMyDid()호출은 DID를 계속 새로 만드나? => 새로 만든다
      !- anonCrypt에 아무 wh나 전달해도 되나? => ecnrypt/decrypt 모두 된다!
      !- test:10 (vcx-conn 메시지 캡쳐) 필요 없음을 README에 명시
    !- clean-up: 반복 테스트, log 제거
      !- common.js, agency-flow.js, *.spec.js, artillery-proc.js
  !- vcx create-agent
  !- wrap-up
    !- 만들어진 client wallet을 open 하는 방식이 아니라 테스트할 때 새로 create 하는 방식으로???
      => wallet 만드는 시간이 꽤 걸린다 (2.5+ sec/wallet)
      => 지금처럼 만들어진 wallet을 이용하는 방식 고수
    !- entity 숫자(faber-N, alice-N) 설정에 따라 onboarding 부하 테스트를 진행하도록 코드 정리
      !- artillery phase 설정에 따라 exception 발생, 이때 뭔가 꼬이는 문제 해결
        => context.vars['users'] 이용, context 관리
      !- artillery test scenario 종료후 onboarding 성공/실패 숫자 확인
        . wallet open 이후 시도한 횟수 체크
        . connect,sign-up,create-agent 실패횟수 체크 (Agency:WalletAlreadyExistsError 제외)
        !- artillery Best Practice 파악
        !- artillery test scenario 종료후 코드 개입시키는 방법? => afterScenario
    !- summary report 출력
      . https://github.com/artilleryio/artillery/issues/160
    !- directory 구조 정리
    !- config.json 분리
    !- README.md 수정
      !- test:10 onboarding/connect message capture 관련 내용 제거
      !- test-config.json 설정
0715- Mediator 성능 테스트 코드 개발: create faber(N개) wallets and alice(N개) wallets
  => indy.generateWalletKey(seed) 호출시 "Indy Error: CommonInvalidStructure" 문제 있었음
  => indy.generateWalletKey() 함수는 RAW 방식으로 wallet-key를 생성하는 함수
  => indy.generateWalletKey()를 호출해서 wallet-key를 만들어
      indy.createWallet(wallet-name, wallet-key, "RAW")를 호출하는 방법을 쓰지 않고,
      indy.createWallet(wallet-name, some-seed-string-as-wallet-key, "ARGON2I_MOD")만 호출하여
      wallet을 생성함
  => 이는 vcx/wrapper/node/demo/faber.js 과 동일한 방식임
  => 생성된 wallet을 some-seed-string-as-wallet-key로 open할 수 있는지 indy-cli로 검증함
0715- vcxagencynode PR 관련 Patrik 요청 대응
0701- VCX Slack의 Patrik 메시지 정독
0701- AbsaOSS/vcxagencynode README 정독
  !- vcxagency-node/ README.md, DEV.md, CONFIGURATION.md
    vcxagency-client/ README.md,
    vcxagency-tester/ README.md
  !- README 수정 (vcx-study, initial-mediator, ...)
  !- to PR
    !- $/vcxagencynode/vcxagency-node/README.md:36
      [here](./configuration.md) -> [here](./CONFIGURATION.md)
    !- $/vcxagencynode/vcxagency-node/README.md:39
      [first need to build base image](../ubuntu-indysdk-lite) -> [first...](../vcxagency-base)
    !- $/vcxagencynode/vcxagency-node/README.md:67
      [VCX Tester](../vcx-tester) -> [VCX...](../vcxagency-tester)
    !- $/vcxagencynode/vcxagency-node/README.md:83
      [overview](./dev.md) -> [overview](./DEV.md)
0624- Mediator 기능 테스트
  !- Jest 학습
  !- Alice&Faber demo with vcxagencynode
  !- Alice&Faber demo with nodeJS lts/erbium
    !- indy-sdk/vcx/wrapper/node/ 에서 lts/erbium 버전의 npm install 실패 원인?
      !- 문제 node-module 파악 => ffi, ref, ref-struct, weak
    !- initial-mediator/feature-test 용 nodeJS 버전 결정 => NodeJS lts/carbon (되는 버전)을 쓰기로
  !- (vcxagency-node unit-test 포함) vcxagencynode 테스트 코드 체크: 참조할 테스트 시나리오가 있는가?
    !- psql 이용, DB 데이터 확인
    !- vcxagency-node test 시나리오중에 VC발급 Proof확인 시나리오가 있는가?
      => vcxagencynode/node-vcx-wrapper/demo 확인하자
      !- 여기도 nodeJS lts/erbium 으로 npm install 에러 (node-gyp 빌드 에러)
        => node module을 잘만들어야 한다
        => NodeJS lts/carbon (되는 버전)을 쓰기로
    !- von-network 적용, ledger 데이터 확인 => vcxangency-node test:unit 은 indy ledger를 쓰지 않는다
    !- noti test:unit 시나리오 확인 (messaging/aries-msg.spect.js)
      => web-hook url을 호스팅하는 서버가 준비되어야 함
      => web-hook 호스팅 서버는 노티 처리를 위해 /notifications path를 제공해야 함
      => agency의 alice-agent에 noti url을 setWebhookUrlForAgent() 설정해주어야 함
        => agency레벨이 아니라 agent레벨에서 설정하는 것이 특이하다고 생각
  !- vcxagencynode/vcx-tester 디렉토리 내용 확인
    => genesis-path는 /vcxagencynode/vcxagency-tester/test/tools/network-registry/genesis/* 에 존재
    => agency-url은 어플소스에 존재 (alice/faber agent를 agency에 provision할 때 참조)
  !- alice-faber w/ von-network, ledger 데이터 확인
    !- mediator는 ledger와 무관한 것 아닐까??? => 무관함 (정기님 컨펌)
    !- vcxagency-node test:unit 실행 => vcxangency-node test:unit 은 indy ledger를 쓰지 않는다
    !- local von-network genesis-path 확인 (ip?) => client_ip, node_ip가 있음
    !- dummy-cloud-agent & von-network & sqlite 에서 alice-faber 확인
      !- 단계별 ledger에 기록되는 데이터 확인
        => SCHEMA, CRED_DEF 2개가 기록됨
      !- revocation 적용시 ledger에 기록되는 TX가 달라지는지 확인
        => REVOC_REG_DEF, REVOC_REG_ENTRY 2개 TX가 추가 기록됨
    !- vcxagency-node & von-network & postreSQL 에서 alice-faber 확인
      !- faber.js, alice.js 수정
        !- "Enter any key" 위치 조절 (polling 직전 코드 제거)
        !- env에 따라 "Enter any key" on/off
  !- vcxagency feature test 시나리오 결정 & 매뉴얼 테스트
    !- alice-faber (/w notification)
    !- scalability
    !- wrap-up test
      !- idle_count를 지정하면 DB failover 시에도 alice-faber 데모가 정상 실행되나?
        => DB table이 유지되면 (DB의 docker volume을 날리지 않으면)
          DB 컨테이너 failover 시에도 데모 시나리오가 정상 실행됨을 확인
    !- README update
      . vcxagencynode 레포를 로컬로 가져온 것 반영
    !- share with team
      . demo/common.js provisionAgentInAgency 코드에 webhook 등록 코드 추가
        => faber.js,alice.js 수정 필요 없음
      . agency 2개 (8080,8081) 띄우고 faber.js와 alice.js의 agency endpoint를 달리 지정해도 데모 성공
        => scale-out 문제 없음
        => DB 재시작시 Agency 정상 동작 않음
      !- next todo 논의 (성능/부하 테스트)
        . (n개 holder, n개 issuer/verifier, 1개 Agency 상황에서)
          . 1개의 issuer/verifier가 여러개의 holder를 상대하는 시나리오를 고려해야 하는지 고민해보자
          제대로 동작하는지 여부를 따지자
        . (n개 holder, n개 issuer/verifier, n개 Agency 상황에서)
          제대로 동작하는지 여부를 따지자
        . 그다음 성능을 따지자
          - provistion agent in agency: 처리속도, max agent 개수, ...
          - issue VC to Alice: 처리속도, ...
          - submit Proof to Faber: 처리속도, ...
      !- faber1 : aliceN 테스트
        - Q: cred-def 를 1번 만들어 재사용하면 에러 "invalide cred-def handle", 이유?
    !- feature-test 소스레포 정리
0624- DB Connection Best Practice 파악 => MM메모함
0615- 태규님 ncloud 확인요청: LB(10.181.23.6) 상태 확인
0609- Mediator 개발 계획
  !- 팀장님 메일 회신: 개발 일정 (~6.9.화)
  !- DummyCloudAgent 분석 자료 공유
    . step-by-step Alice&Faber, agent-conn 3개의 생성시점 확인
      . nodeJS version 확인 (v8.17.0)
      . check ~/.indy_client directory & wallets
      . check http://localhost:8090/admin, /forward-agent
    !- Q: Mediator end-point가 2개(8080,8081) 이유? => skip
    !- Q: DID,VerKey (WalletName,DID,SeedScret,WalletKeySecret, ...) 어떻게 만드나?
      => http://54.180.86.51/ 에서 "Register from seed" 하면 생성됨
  !- NodeJS Mediator 평가 항목 (== Mediator 개발 목표)
    . 기본 기능이 제대로 구현되어 있는가?
      . Faber-Alice 데모 테스트
      . Single Issuer/Verifier/Holder 테스트
      . Multiple Issuer/Verifier/Holder 테스트
    . 단말이 오프라인일 경우에도 연결성을 보장하는가? (noti를 지원하는가?)
    . mediator가 state-less한가? (scale-out 가능한가?)
    . 성능 테스트
      . 시스템 Failure 발생 시 정상 서비스 여부
      . 과부하 Stress 테스트
    - unit/regression test tool?
    - load test tool?
  !- Mediator 관련 todo
    - 운영 서버 설정 (상용 4대, 테스트용 3대. TDE?)
      - Docker/Swarm 설정
      - 방화벽 Any 오픈 (8080,8081?)
      - DNS 등록 (initial-agency.sktelecom.com?)
  !- CloudAgent R&R?
    - 윤지혁,김인기 => Mediator 운영 (개발 항목 미지수)
    - 성백재,김정기 => CloudAgent REST API 개발 (SteetCred 기준 Agency-API와 Wallet-API)
  !- Mediator 일정
    - Mediator as-is 기능 평가& 성능 평가 : 1달 ~6.E
    - Mediator 부족 부분 개발 : 1달 ~7.E
    - 테스트 인프라& 운영 인프라 구축 (Swarm/K8s) : 1달 ~8.E
    - Mediator QA : 1달 ~9.E
    * 변수: Initial Ledger NW 확장 (6월 중순. 약 2주 소요)
0608- NodeJS Mediator 릴리즈 내용 확인
  !- 테스트코드 실행 => yarn run test:unit
  !- 데모 실행 (production mode) => yarn run serve
  !- clean-up
    !- Dummy-cloud-agent 실행 확인
    !- AWS 중계 서버 stop
0603- MVCC? (같은 키로 TX를 많이 날릴 경우 race-condition? 대응 기법) 개념 체크
  => DB의 concurrency-control 기법 중 하나
  => DB 데이터를 읽을 때 버전 분기를 허용 (즉 multi-version을 허용)
  => (소스코드 버전 컨트롤처럼) DB 데이터의 충돌이 발생할 수 있으며, 이를 Application이 해결해야 함
0603- CloudAgent: libVCX & dummy-cloud-agent 확인 (데모를 중심으로...)
  => 7월말까지 alice/faber cloud-agent PoC 데모 필요
  !- 데모 (demo:faber, demo:alice) 분석
    . https://github.com/hyperledger/indy-sdk/tree/master/vcx/wrappers/node
    !- demo:faber, demo:alice 실행 => ~/work/indy-sdk-work/README.md 작성
    !- agency/mediator 개념 파악
      . https://github.com/hyperledger/aries-rfcs/blob/master/concepts/0046-mediators-and-relays/README.md
    !- demo:faber/alice & dummy-cloud-agent/libvcx flow 분석
      !- DummyCloudAgent 실행 없이 데모를 실행하면?
        => "#0 Initialize rust API from NodeJS" 성공
        => "#1 Config used to provision agent in agency:"에서 실패
        => demoCommon.provisionAgentInAgency() 호출에서 실패한 듯 (Agency가 응답하지 않아서)
      !- faber.js -> libvcx -> mediator 체크&메모
      !- dummy-cloud-agent 8081 포트로 데모 성공하는지 체크 => 성공함
      !- faber 로그 체크 & 의문점 도출
        !- institution_did? faber의 edge agent did와 다른 놈인가?
          => schema를 만드는 조직인 듯. 중요하지 않다고 판단.
        !- connection, credential, proof 모두 vcx_connection_create() API로 만든다. 왜 그런가?
          모두 connection인가? dummy-cloud-agent의 AgentConnection과 관련 있는가?
          => tag trace 실수
          => 각각 vcx_connection_create, vcx_issuer_create_credential, vcx_proof_create으로 달리 만들어짐
        !- pairwise DID는 왜 pairwise라고 표현했는가?
          . https://www.evernym.com/wp-content/uploads/2017/07/What-Goes-On-The-Ledger.pdf
          => 혹시 connection 양단에 각기 다른 2개의 DID를 부여해서 그런 것 아닐까?
          !- connection 목록 조회 vcx api?
            또는, 최소 특정 connection 정보(did, verkey, ...) 조회 vcx api?
          !- demo snapshot 새로 캡쳐
          !- dummy-cloud-agent 객체간 관계 지도 작성
          !- node_vcx_demo_faber_wallet과 alice_wallet은 DCA 안에 만들어지나? 밖에 만들어지나?
            => DCA 밖. common.provisionAgentInAgency() 코드 안에 지갑 만드는 코드가 별도 존재한다 (성백재님)
            => libvcx에서 agency에 onboarding 메시지를 던지기 전에 wallet을 만든다
              indy-sdk/vcx/libvcx/src/messages/agent_utils.rs:272 connect_register_provision() 중
        !- forward-agent-connection과 agent-connection의 역할? 뭐가 다른가?
          => forward-agent-connection은 edge-user와 해당 edger-user의 agency/agent 사이의 연결을 담당 (한다고 추정)
          => agent-connection은 edger-user와 상대편측 agency/agent 사이의 통신을 담당 (한다고 추정)
            => 통신 수립을 위해 1개, 통신 방향에 따라 2개, 토탈 3개 conn 필요
    !- 오픈소스 PR
      !- vcx/wrappers/node/README : typo fix
      !- vcx/dummy-cloud-agent/docs/admin-api.md : API url fix
  !- Slack did 채널 김정기님 pinned 메시지 (vcx wapper들, 백제님 Java 데모, vcx api 문서) 체크
0602- Initial, SDS 메일 대응
0602- Initial, 하나은 메일 대응
0527- 블록체인 개발팀 워크샵 자료 (Mediator)
0526- @vm6, vim, js ctag 설정
0526- Initial SDS 문의 대응
  !- initial nw 접속 권한 요청 (ncloud?) to 송지영님
  !- Q?
    !- 테스트넷 운영중인가? => OK. 하지만 추가 확장 계획 없음
    !- 그라파나 적용중인가? 접속정보? => 메인넷만 운영중
    !- 참여사 웰컴 패키지 (테스트넷 접속방법, API 사용법 문서, ...)? => Slack Q/A
0520- Azure 900 (기초교육)
  . 참조자료: http://ilseokoh.com/azure-virtual-machine-scale-set-2-120-instances/
0518- long-term-test 설정&운영
  !- log 확인
    . orderer: 172.27.19.109, 83, 55, 32
    . peer: 172.27.26.31,104, 33,30, 80,96
  !- grafana 확인
  !- 공지
    . 5.18~ docker-volume 처리, docker-log 처리 보완하여 HLF1.4 long-term-test 재시작함
0515- HLF long-term test 이상현상 파악: grafana 페이지 통신 오류 발생
  - 원인?
    => docker storage full
    ```
    [svcapp_su@pdv-hlft-os003 ~]$ df -h
    Filesystem      Size  Used Avail Use% Mounted on
    /dev/vda1        20G  6.0G   13G  33% /
    devtmpfs        3.9G     0  3.9G   0% /dev
    tmpfs           3.9G     0  3.9G   0% /dev/shm
    tmpfs           3.9G  394M  3.6G  10% /run
    tmpfs           3.9G     0  3.9G   0% /sys/fs/cgroup
    /dev/vdb1        50G   38G  8.8G  82% /var/lib/docker
    tmpfs           799M     0  799M   0% /run/user/520
    tmpfs           799M     0  799M   0% /run/user/1801
    ```
  - 현상파악
    1. zk01 만 재시작 (zk01의 상태만 restart 였음) => grafana 복귀 안됨, 1일 기다려보기로 함
      ```
      sudo truncate -s 0 /var/lib/docker/containers/[container-id-of-zk01]/[container-id-of-zk01]-json.log
      docker prune --volumes -f
      docker-compose -f docker-compose-172.27.19.109.yaml up -d zk01 kafka01 orderer1.ordererorg1.com
      ```
    2. kafka cluster 재시작 => query-cc.sh 동작 확인함, 회사망에서 grafana 확인하기로 함
    3. grafana container 재시작후 run-cli.sh의 grafana 초기화 스크립트 재실행...해보려던 참...
      - grafana 초기화 스크립트 의미/동작/... 확인 (w/ 김성택님)
    4. --- wati a moment! ---
      . query-cc 성공하나? => yes
      . invoke-cc.sh 성공하나?, 즉 HLF 네트워크가 정상 동작하나? => no
       ----------------------
      => 1번 과정에서 orderer1.ordererorg1.com (172.27.19.109 머신의 orderer) 만 죽였다 살렸다
      => orderer가 죽었다 살면 다른 peer(아마도 다른 orderer)로부터 상태를 복구한다
        . /var/hyperledger/production/orderer/chains/skt-sys-channel,mychannel 아래 블록파일들
        . 블록파일 복사에 시간이 많이 걸림
      => invoke-cc.sh 실행시 orderer1.ordererorg1.com의 블록높이가 다른 peer들과 달라 합의에 실패한다고 추정
      - 모든 블록파일이 복구될 때까지 기다려보기로 함 (orderer1.ordererorg1.com, 04.28.~ 며칠 걸릴 것 같음)
        !- 모든 peer를 stop 시킴 (04.29.07:30. p2p 블록파일 복사 출처가 orderer인지 peer인지 확인하기 위해)
          => peer들을 docker rm 한 것이 아니기 때문에 docker volume은 유지되고 있음
          => orderer1.ordererorg1.com 의 로그가 중단됨 (p2p 블록파일 복사의 출처가 peer인 것으로 추정)
          !- peer1,2.org1.com, peer1,2.org2.com restart (04.29.08:23)
            => orderer1.ordererorg1.com 의 로그가 재개됨. 하지만 블록파일의 크기에는 변동 없음
            => 에러 메시지:
  2020-04-28 23:23:05.742 UTC [common.deliver] deliverBlocks -> ERRO f138 [channel: mychannel] Error reading from channel, cause was: NOT_FOUND
            => peer1.org1.com, peer1.org2.com (앵커피어)만 orderer1.ordererorg1.com에 접속하는 것 같다
          !- peer1.org3.com restart (04.29.08:40)
          !- peer2.org3.com restart (04.29.08:57)
            => NOT_FOUND 에러가 사라지고 블록파일의 크기가 증가하기 시작함
            => peer2.org3.com 이 범인이 아니라
              채널 소속 모든 (앵커)peer가 살아나는 시점에 블록파일을 수신한다고 추정함
        !- 모든 (앵커)peer를 stop시키고 아까와 반대 순서로 (앵커 peer만) restart 시켜보자
          peer1.org1.com이 붙는 시점에 블록파일 수신이 재개되면 추정이 옳다
          !- peer1.org1.com stop
            => peer 1개만 stop시켜도 NOT_FOUND 에러가 재발함, 블록파일 복구 멈춤
          !- 모든 앵커 peer stop (peer1.org1.com, peer1.org2.com, peer1.org3.com)
            => 모든 앵커 peer만 stop시킬 경우 NOT_FOUND 에러 반복됨 (peer2들이 orderer에 접속하며 로그를 남김)
          !- 모든 peer stop
            => orderer1.ordererorg1.com의 (모든) 로그 멈춤
          !- 모든 앵커 peer restart (peer1.org3.com => peer1.org2.com => peer1.org3.com)
            => peer 1개만 restart해도 orderer 로그 시작됨, 블록파일 복구는 여전히 멈춰있음
            => 모든 앵커 peer를 restart해도 NOT_FOUND 에러 반복됨
          !- 모든 peer restart (peer2.org3.com => peer2.org2.com => peer2.org1.com)
            => 마지막 peer를 restart 하자마자 NOT_FOUND 에러 사라짐, 블록파일 복구 시작됨,
              각 peer들의 cc 컨테이너 살아남
        => 결론, 블록파일 복구는 채널에 소속된 peer들로부터 orderer1.org1.com에 전달됨
      !- orderer에 볼륨마운트 설정한 다음 복구속도 비교 (볼륨마운트 있을 때 vs 없을 때)
        !- orderer1.ordererorg2.com 강제 종료후 볼륨마운트 설정하고 재시작 (05-06T15:43~)
          => docker-compose-172.27.19.83-vol.yaml 에 볼륨마운트 설정 반영함
          => /var/hyperledger/productoin/orderer/chains/mychannel 에 블록파일 복구 시작함
            (볼륨마운트 없을 때보다 빨라진 느낌)
          => 처음 복구 속도는 2배이상 빨랐으나 점차 똑같아짐 (볼륨마운트 때문에 빠른 것이 아니었음)
        !- 볼륨마운트가 있는 상태에서 orderer 컨테이너를 죽였다 살리기
          ``` bash@172.27.19.83
          docker rm -f orderer1.ordererorg2.com
          docker volume ls
          docker-compose -f docker-compose-172.27.19.83-vol.yaml up -d orderer1.ordererorg2.com
          ### check /var/hyperledger/production/orderer/chains/mychannel
          ```
          !- 볼륨(/var/hyperledger/production/orderer/...)이 유지되는가?
            => OK! 볼륨이 유지되며 볼륨 내용을 기반으로 복구 시작함
      !- 현재 tx 부하 발생을 위해 1분마다 실행되는 crontab 설정을 변경해보자
        !- 1년에 1번으로 변경하고 나서 orderer에 전달되던 블록파일이 끊겼다
          => peer가 orderer를 찾지 않기 때문인 것으로 추정됨
        !- 1분마다 invoke가 아니라 query하도록 설정을 변경해도 orderer에 전달되는 블록파일이 끊겼다
          => query는 orderer를 찾지 않기 때문인 것으로 추정됨
        !- 1분마다 invoke하도록 복구
        - 지금 orderer1.ordererorg1.com과 orderer1.ordererorg2.com에 쌓이는 블록파일이 복구 파일일까?
          혹시, invoke로 인해 새로 생성되는 파일들 아닐까?
          . orderer1.ordererorg2.com 신규 시작시 블록파일들이 빠르게 복사됐던 것을 기억해보면
            블록파일이 복구되던게 맞는 것 같다
          . crontab 으로 invoke를 멈출 때 블록파일 복사/생성이 멈추는 것을 보면
            실패 TX로 인해 새로 생성된 블록파일일지도 모르겠다
          - orderer1,2,3 블록파일 비교
!!!
            - 블록파일 inspect tool?
              !- protoc --decode_raw < blockfile_000000 => error
              !- configtxlator proto_decode --input blockfile_000000 --type common.Block => error
!!!
          - 정상운영 중인 orderer3와 orderer4의 블록파일 개수가 다르다
            ```bash @orderer1.ordererorg3.com
            root@459af0f575f9:/var/hyperledger/production/orderer/chains/mychannel# ls -l |grep ^- |wc -l
            333
            -rw-r----- 1 root root 28452716 Apr 24 00:51 blockfile_000332
            ```
            ```bash @orderer1.ordererorg4.com
            root@a18eeffba92d:/var/hyperledger/production/orderer/chains/mychannel# ls -l |grep ^- |wc -l
            332
            -rw-r----- 1 root root 64043195 Apr 24 00:51 blockfile_000331
            ```
            - o1,o2,o3,o4의 처음 블록파일(blockfile_000000) 비교
        - 복구중인 o1,o2 과 운영중인 o3,o4 특이 사항
          . o1 만 실행시켜도 (o2는 죽은 상태에서) peer들 에러로그가 사라짐
          . o1은 "Error reading rom channel, cause was: NOT_FOUND" 에러로그가 가끔 발생,
            그런데 o2는 해당 에러로그가 계속 발생
        - orderer1,2를 stop 시키고 orderer3,4가 동작하는지 확인 (5.12 09:10~ 5.13 09:52)
          - peer1.org1, peer2.org2, peer1.org3에서 orderer를 찾지 못했다는 에러가 발생,
            다른 peer들에서는 에러 없음,
            genesis block에 포함된 orderer-list를 써서 다른 orderer를 시도하지 않는 이유?
            (왜 orderer1,2만 찾나?)
  !- 현상 파악후 조치
    !- log-max, log-rotation 설정
      . 컨테이너 레벨 옵션 지정 필요 => host 레벨 docker 설정에 대한 su 권한이 없을 경우 대비
    !- volume mount 설정
      . orderer/peer의 파일 i/o 처리 속도 향상을 위해 필요
      . orderer/peer 장애후 복구시 빠른 상태 회복을 위해 필요
      . https://github.com/hyperledger/fabric-samples/blob/release-1.4/first-network/base/docker-compose-base.yaml
      . volume mount 대상
        - zk:
          - /data
          - /datalog
        - orderer:
          - /var/hyperledger/production/
        - peer:
          - /var/hyperledger/production/
        - couch:
          - /opt/couchdb/data
          - /opt/couchdb/etc/local.d
        - prometheus:
          - /prometheus
        !- cli: => skip, 다음에
          - /etc/hyperledger/fabric
      !- dockercompose/writer1x4 적용
        !- docker-net
        !- host-net
      !- dockercompose/writer1x0 적용 ==> skip
  !- 현상 및 현상 파악후 조치 내용 공유
    . 장애 현상과 조치 결과 (우아하게 정상화 성공?)
    . 조치 항목 및 skt-hlf 툴 반영 내용 (v1.0.515)
      . docker log 관리 옵션 적용 => log 파일로 인한 storage full 상황 방지
      . container volume 적용 => disk io 속도 향상, 컨테이너 장애/종료시에도 기존 데이터 보존
        (HLF1.4 에만 적용, HLF1.0 적용은 skip함)
0513- streetcred.id 분석
  !- streetcred.id 데모 실습
  !- streetcred.id 개발문서 학습
  !- aries API spec 체크 (streetcred와 뭐가 다른가?)
    !- aca-py 문서 확인
      !- https://github.com/hyperledger/aries-cloudagent-python/blob/master/AdminAPI.md
      !- https://github.com/hyperledger/aries-cloudagent-python
    !- aca-py 실행환경 설정 & 실행 & Swagger 페이지 확보
      => PORTS="8080:9000 8000:8000" \
        ./run_docker start --admin 0.0.0.0 9000 --admin-insecure-mode \
        --inbound-transport http 0.0.0.0 8000 --outbound-transport http --log-level DEBUG
      => Swagger URL: http://172.27.26.112:8080
    !- aca-py API 분석
      !- demo code 분석
      !- test code 분석
      !- 실행체 deploy 방식 비교: 정상 동작하려면 aca-py를 어디에 몇개나 deploy 해야 하는가?
        => aca-py & controller를 같은 곳에 deploy 해야 한다
        => aca-py와 controller는 1:1 이다
      !- aca-py API vs StreetCred API 비교
    !- aca-py를 이용한 certification & verfication 플로우 검증
      . https://github.com/hyperledger/aries-cloudagent-python/blob/master/demo/AriesOpenAPIDemo.md
0512- 업무 현황 보고 자료
0508- IP마당 요청 대응 => IPR팀에서 대응완료
0508- 송지영m님 메일 대응: DID 인프라 비용 낮추기 => BD 이재학님 처리하기로
0428- 강태규님 Initial Infra 인수인계
  => (T-DE 08.Infra/Initial)[https://tde.sktelecom.com/wiki/display/CHAINTECH/Initial]
0424- BaaS: AWS,Azure 담당자 컨택
  !- 3월말 Stonledger 홍보페이지 구축상태 확인
  !- 3월~6월 사이 홍보페이지 이용 가능할 때 AWS,Azure 담당자 컨택
    . 인사
    . 마켓플레이스 추진 절차, 일정 문의
  !- 회의
    . 리스팅 등록, VM이미지 등록, SaaS 등록 방법/절차 확인
0421- 브랜드사이트 리뷰
  . https://tde.sktelecom.com/stash/projects/STONBRAND/repos/stonledger/browse
  !- 사이트 개발
    !- IE로 보면 첫머리에 깨진 이미지박스가 보인다
      => hero-imagelarge.png 파일명 대소문자 혼재가 원인
  !- firebase 사용법 확인
    !- stonledger@sk.com 계정 연동
    !- firebase 파일 목록 관리 방법 파악 (w/ 김성택님)
  !- stonledger@sk.com 메일함 연동 신청
0421- 회사 태스크 등록
  !- DID 네트워크 구축 태스크의 경우 형주님과 병수님을 task-mate로 등록하자 (팀장님 지시)
  !- 브랜드 사이트 구축 태스크에 윤지혁님을 task-mate로 등록하자
0416- streetcred.id 협의 (기초 개념)
  !- ledger 용도는?
    => issuer의 pub-key가 기록됨 (issuer는 아무나 할 수 없음)
      => verifier는 VC에 기록된 issure 정보를 보고 ledger 조회를 통해 검증
    => VC의 schema가 기록됨
      => verifier는 ledger에서 조회환 schema로 VC의 데이터를 해석
    => VC에 대한 revocation registry가 기록됨 (issuer가 발급)
  !- StreeCred를 쓰면 issuer/verifier의 SI 부담이 덜어지는가?
    => 아니다. SI는 필요하다.
  !- Why Blockchain?
    . 블록체인 없이는 DID가 구현 안되나?
    . 블록체인을 써야 하는 DID 나름의 이유/철학이 있나?
  !- 범위&마일스톤
    . https://tde.sktelecom.com/wiki/display/CRISIS/Indy+Anocreds+Workflow
    => REST API 정의
      . api 호출을 위한 인증체계 필요 => TID 연계
    => frontend 화면/시나리오 정의
    => 9월말까지 PoC
    => 소위과제 (그룹자 출입관리) 돈 나오고 6개월 (11월 완료 예상)
0416- skt-hlf: peer/orderer volume 관리 추가
  . docker-compose에 peer/orderer를 위한 named volume 추가
  . peer/orderer를 docker rm -f 한 뒤에
    docker-compose peer/orderer up -d 하면 이전 상태를 회복함을 확인
    (peer/orderer의 .../production/... 디렉토리 확인)
  => 살펴보니 peer와 orderer에는 volume 마운트가 없음 (MSP 디렉토리에 대한 bind 마운트만 사용)
  => couchDB는 volume 마운트 2개가 있음 (/data, /datalog)
  => peer/orderer 재기동시, p2p 통신을 통해 최신 state를 따라가는 것이 좋겠다고 판단함
  => 작업 계획을 취소함
0413- eco: 작업목록 리뷰
0413- skt-hlf-rest: REST API 개발 2차
  !- swagger in-depth
    . yaml을 입력받아 tar를 리턴하는 API를 swagger로 어떻게 서술하나? (/hlf)
    . json을 리턴하는 API를 swagger로 어떻게 서술하나? (/eco-member)
    . authorization을 swagger로 어떻게 서술하나?
    !- Open API Spec
      - https://github.com/OAI/OpenAPI-Specification/blob/master/versions/2.0.md
      - https://github.com/OAI/OpenAPI-Specification/blob/master/versions/3.0.3.md
    !- Swagger Spec: https://swagger.io/docs/specification/
    !- Swagger 코드 생성기 검토: 생성된 코드가 쓸만해 보이는가?
      !- oauth2 코드도 생성되는가? 쓸만한가? => oauth 코드는 생성되지 않음
    !- Swagger 초안 @Swagger.io
  !- swagger 서술 개선
    !- swagger doc을 소스파일에 분산시키는 것이 가능하고 적절한가?
      => 하나로 몰자 (Swagger 웹서비스를 비롯 다른 툴을 쓰기가 좋을 것 같다)
    !- swagger yaml을 통한 swagger 페이지 노출 구현
    !- tar 리턴 API의 Swagger 서술은 적절한가? => maybe
    !- localhost:8080/api-docs URL은 적절한가? => /api/v1/docs 로 하자
  !- yamljs, js-yaml 통합 가능성 확인 => js-yaml 로 통일 (크기 작고, 동작 확실하고. 자주 업데이트)
  !- 구조 개선 (model, controller, router, middleware, ...)
    => 다음에 하자. 지금은 파일을 분할하면 오히려 코드 가독성이 나빠질 것 같다 (실질 api 1개)
  !- tar 리턴 API 개선: 파일을 리턴하는 코드는 적절한가? => 적절하다
  !- https 적용: let's encrypt? => 정보보안포탈! => 브랜드 사이트를 통해서 하자
  !- 인증 처리: API key? JWT (JSON Web Token)? => 나중에 하자
  !- version 갱신 (@dev), master merge
  !- 공유
    . Swagger Page URL (.203 사내 유무선망에서 접근 가능)
    . /members API 테스트 방법 설명 (w/ 화면캡쳐)
    . 프로비저닝웹 URL (.203 사내 유무선망에서 접근 가능)
0409- 태규님 인수인계
  !- gocd (인프라 계정, 작업 매뉴얼)
  !- baas (인프라 계정, 작업 매뉴얼)
  !- did network 서버 리스트 (인프라 계정, 작업 매뉴얼)
  !- public ip 서버 사용 방법 의논
0408- skt-hlf: CA 검토
0408- 팀 경영계획 재검토
  !- 무엇을 어디까지 해야 하나? => DID Cloud Agency 집중
  !- 올해 성과 지표를 무엇으로 할 것인가? 팀 KPI와 어떻게 연계시킬 것인가? => 국책과제 연계중
  !- DID Cloud Agent 파악
    . doc: https://github.com/hyperledger/aries-rfcs/tree/master/concepts/0004-agents
    . ref: https://streetcred.id , https://developers.streetcred.id
  !- STON Asset Depository App 현황 파악
    . wiki: https://tde.sktelecom.com/wiki/display/ZERODBNNGO
    . repo: https://tde.sktelecom.com/stash/projects/ZERODBNNGO
0406- 성택님 인수인계
  !- 성택님, 브랜드페이지: https://tde.sktelecom.com/stash/projects/STONBRAND/repos/stonledger/browse
    => 4/1 팀 위클리에 관련 메모 있음
    . 엔트리파일은 main.vue
  !- 성택님, 브랜드페이지 & SK open API 협업 진행현황 (컨택포인트, 일정계획, ...)
    . 윤철중님(sys), 장현춘님(sys), 김정선님(사업)
    . 브랜드페이지 저쪽 인프라에 얹어 운영하기로 함
    . 6월 9월 오픈 공감대 있음
  !- 성택님, 브랜드 머티리얼 (도메인, 브랜드, ...)
    . comm.> 도메인 조회/연장 메뉴에서 신청자 이름(김성택님)으로 조회
      . stonledger.co.kr, .net, ...
    . 인증서 확보해야 함
    . 메일 계정 stonledger@sk.com 확보함
      . 전자결재> 신청서 및 요청서> IT요청서> 요청유형/기능개선/인트라넷,메일,계정,SSO
  !- 성택님, 약관 진행현황
    . 리갈넷에 결재 (팀장님,원장님 책임자에 있음을 먼저 보고해야)
    . 회원 가입시 약관 동의 받을 때, 만14세 확인 체크 표출 필요
  !- 성택님, 백서
    . 세세한 내용은 날리고, value added 내용만 서술
  !- 성택님, Ston App 내용 주재현님에게 백업함
  !- 성택님, ref 사업 현황
    . 가톨릭 중앙의료원: 헬스케어CoE, DID 사업팀과 공유 (팀장님도 알고 계심)
  !- 브랜드 페이지 협의 (w/ 주재현님,김성택님,윤지혁님)
    . 브랜드 페이지를 통한 DApp 샘플 공개 협의
    . STON Asset Infra 관리 협의
0324- skt-hlf: 서버 다양성 fix
  . 현상: $PWD에 사용자id가 포함되어 있지 않을 경우, 작업 디렉토리가 존재하지 않아 실패
0323- baas: stonledger 브랜드 페이지 피드백
  !- BaaS 소개 문구
0323- BaaS 기획 리뷰
  !- Channel, Consotium 개념을 합칠 것인지? => 합치자
  !- DID 기능 올해 안에 할 것인지? => 우선순위를 낮춰서 하자 (주재현님쪽 작업 결과물을 수용하자)
  !- HLF 을 전면에 내세울 것인지? => 전면은 아니더라도 작게 명시하자
0323- HLF in-depth
  !- Work-book
  !- BYFN, Adding Org, CA tutorial
0316- skt-hlf: 프로비저닝 사전 검증 도구 테스트
  . 사전 검증 도구 사용법 체크
  . HLF 없는 상태에서 사전 검증 수행
  . org2/peer1 컨테이너를 내리고 프로비저닝 하기전 사전 검증 수행
  !- @dockernet 테스트
  !- @hostnet 테스트
  !- 피드백
    * rendezvous 명령어를 rdv로 단축하면 좋겠음
    * rendezvous copy 명령어를 cp로 단축하면 좋겠음 (rm과 같은 맥락에서)
    * Port open OK인 경우, http response가 기대와 다르더라도 ERROR 보다 부드러운 말(WARN?)로
    * docker image가 발견되지 않을 경우 ERROR 보다 부드러운 말(WARN?)을 쓰면 좋겠음
    * docker image 체크 루틴과 network 체크 루틴을 분리하면 좋겠음
      . 지금은 같은 서버에 대한 docker image 체크가 반복되고 있음
      . 체크하는 서버에 어떤 HLF 요소가 올라가는지 (zk,kafka,orderer,peer,...) 표시되면 좋겠음
  !- dev 머지
    !- main_test.go 에 테스트 시나리오 추가 => skip. rendezvous는 외부 툴, 이번엔 skip하자.
    !- ston-image 레포에 skt/rendezvous 이미지 등록
  !- 팀 공지
    !- 신규 기능 rendezvous
      . 작업자: 윤지혁님
      . 프로비저닝 준비 상태(network, docker-image) 테스트
    !- test-1.4/rdv-*/README.md 참조
0309- skt-hlf-rest: REST API 개발 1차
  !- REST API 개발 베스트프랙틱스 파악
  !- Swagger 문서
    !- Hello API 추가
  !- 팀 공지
    !- Swagger 문서 URL
0304- HLF deep-dive 공유: Cloudsoft AMP Hyperledger Multi location Demo
  . https://www.youtube.com/watch?v=jnSMjfVKgEk&list=PLfuKAwZlKV0_--JYykteXjKyq0GA9j_i1&index=8
  . 클라우드(AWS?)에 존재하는 멀티 데이터센터에 HLF를 배포/생성/운영하는 2017년 시연 동영상 (3분)
  . BaaS 기획에 참조할만
0226- skt-hlf: target server 관리 기능 추가
  . 계정이 다른 서버들 (즉, $HOME 디렉토리가 다른 서버들) 지원
  . ssh port가 다른 서버들 지원
  !- get-*.sh, ... 수정안 마련
  !- hlf-config.yaml 수정
  !- get-*.sh, ... 수정안 반영
  !- SSHInfo -> SSHOptions
  !- dev merge
0220- skt-hlf: cc-amdin
  !- 구현 방안 설계: job yaml, master sh, ...
  !- cc install 기능 추가
  !- cc upgrade 기능 추가
0217- BaaS: landing page, PoC page 등록 일정 확인 (SK open API) => 성택님
  . 3월 비공개 컨텐츠 등록
  . 6월 page 공개
0213- skt-hlf: fix master/dev
  !- skt-hlf addorg/ch/peer 명령의 --generate-only 파라메터를 인지 못함 => 리빌드
  !- skt-hlf rebuild
  !- generate-only 파라메터에 대한 unit test 추가
  !- metadata 수정 v1.0.213
  !- regression test 결과 확인
  !- tag 지정 v1.0.213
0211- skt-hlf: regression test 자동화 => v1.0.211
  !- implement Makefile for dockernet-* tests
  !- implement Makefile for hostnet-* tests
  !- git commit
    . check Kafka ready for Kafka-orderer (instead of simple sleep)
  !- merge dev-branche & merge master-branch & release
    !- merge dev-branch: ingee/test-automation, dr.jhyun/proble
    !- merge master-branch
      - metadata 버전 업그레이드
      - skt-hlf 빌드본 포함
    !- 팀내 공지
      . test 자동화
        . 태규님, CI/CD pipeline 구축 (dev 브랜치가 수정되면 regression test 자동 실행)
      . 프로비저닝 준비 상태 체크 도구
        . 지형님, probe 도구 개발 (네트워크 연결 상태, 도커 이미지 준비 상태 체크)
      . skt-hlf version 명령 소개
      ---
      $ ./skt-hlf version
      skt-hlf:
       Version: v1.0.113 => v1.0, 1월13일 빌드 버전
       Commit SHA: development build
       Go version: go1.13
       OS/Arch: linux/amd64
      ---
0211- skt-hlf: 프로비저닝 준비 상태 체크 도구 개발 => 윤지혁님, v1.0.211
  !- 통신 성공 여부 체크 도구 개발
    . hlf-config.yaml 에 명시된 각 머신/포트에 접근 가능한지 확인
    . 각 머신에 웹서버를 띄우고 컨테이너 이름등을 전송하게 구현
    . 요구사항: provisioing 작업을 위한 통신 체크 필요
    . 요구사항: add-org 작업을 위한 통신 체크 필요
    . 요구사항: add-peer 작업을 위한 통신 체크 필요
  !- HLF docker image load 상태 체크 도구 개발
    . ccenv:latest, ... 등 필수 이미지 존재 여부 체크
0210- 경비 처리 (1.15.수요일 저녁식사 w/팀장님,지혁님)
0210- 팀 6 page 검토 & 피드백 (to 팀장님) => 김성택님께 위임
  !- 예산 취합
    !- BaaS: 절차대행/유지보수 0.1억(옵셔널), 개발/테스트용 클라우드 0.1억(필수)
0207- skt-hlf: "ch deploy전 container ready 상태"를 event로 처리
  => cli-script/deploy-all.sh 에서 zk[0]에게 모든 kafka가 broker-id를 등록했는지 쿼리하여 검증
0207- skt-hlf 개발 P.CL 설정 => README_pcl2.md 갱신함
  . 2Core/4G-MEM/30G-STG 3ea, 2Core/4G-MEM/30G-STG 1ea
0205- skt-hlf: grafana page 개선 => 김성택님 위임
  . 요구사항: peer,orderer 별 운전 상태 표시 (초록색, 빨간색)
0204- skt-hlf: provisioning-portal UX 시나리오 합의 => 강태규님께 위임
0204- skt-hlf: admin-portal UX 시나리오 합의 => 강태규님께 위임
0204- skt-hlf: jenkins를 이용한 regression test 자동화 => 강태규님께 위임
  . as-is (ingee/test-automation 브랜치)
    . 한일: README.md => Makefile, test-1.4/Makefile, test-1.4/test-all.sh
    . 목표: 10번 테스트하면 10번 성공하는 코드
      . time-out 조정, invoke-cc 2번, ...
  . 요구사항: dev 브랜치가 변경되면 regression test 자동 실행후 결과 노티
0204- skt-hlf todo-list 공유
  !- Host vs Docker-bridge vs Swarm-overlay vs K8s-overlay NW 성능 비교 결과 공유
  !- go1.13, 소스레포 위치 재확인
    . skt-hlf version 명령, 버전체계 공유
    . go.mod 공유
  !- 작업항목, 요구사항 공유
  !- 필요성, 우선순위, R&R 설문/협의
0129- docker: concept renewal
0123- BaaS 관련 MEC 협업 미팅
  !- BaaS 계획 설명 및 질의 (BaaS 측에서 무엇을 준비해야 하나?)
  => MEC 테스트베드 이용 추진
  => MEC Application On-Boarding 프로세스 진도/내용 추적
0122- skt-hlf: fix-stargate-config
  !- stargate config yaml의 url을 domain-name-addr로 표시 (github의 fabric-sdk-go 모범답안 참조)
  !- docker-compose stargate 항목에 extra_hosts 등록
  !- test-1.4/dockernet-multi-orderer-stargate 등록
  !- test-1.4/hostnet-multi-orderer-stargate 등록
  !- comm with st.kim
    - analyzeHostTarget() 코드를 analyzeConfig()로 옮겨 병합하는 건 어떨까?
    - dash-board yaml 상수들을 dockercompose/writer_1x4_dashbd.go 로 분리함 => 가독성 개선 차원.
    - 소스코드의 역삼각형 문자를 제거함 => 컴파일 에러 가능성 제거 차원.
0121- BaaS 관련 Azure 마켓플레이스 매니저님 미팅
  - 시작 방법?
  - 소요 비용?
  - Azure 컨택 포인트?
0121- 김지성님 VM 접속 확인 (서비스명: 블록체인)
0121- skt-hlf: fix-admin-script
  !- addorg,addch,addpeer 명령의 --test 옵션을 --generate-only 로 변경
  !- addorg fix
    !- run-cli/run-org3-cli.sh docker-nw 없을 경우 생성하는 코드 필요
    !- run-cli/run-org3-cli.sh log-level info로 조절
    !- --out 지정하지 않더라도 일은 성공하게
    !- 기존 peer/orderer (신규 org peer 말고) extra_hosts 처리
      !- docker-compose 에 기존 peer/orderer extra_host 없어도 query-cc,invoke-cc 성공.
        run-org3-cli.sh 에서 기존 peer/orderer 관련 extra_host를 제거해도 성공할까?
        => test-admin 의 경우 1머신에 모든 Org peer/orderer가 존재
        => 신규 Org peer(Org3)가 dockernet (docker0 브릿지)를 통해 gossip 성공했음
        => 신규 Org peer가 다른 머신에 프로비저닝될 경우 기존 peer/orderer extra_host 등록 필요
      !- docker-compose,run-cli/run-org3-cli.sh 에 기존 peer/orderer extra_host 추가
        !- stargate config yaml은 왜 생기나? 설정한 바가 있나? => 설정했다!
        !- docker-compose extra_host에 모든 peer/orderer를 명시하면
          peer1.org3 로그에 접속실패 워닝이 없어지나? => 없어진다!
        !- 신규 Org에 IP가 다른 여러개의 peer들이 있는 경우 고려,
          dockerComposeWriter.writeYamlForSpecifiedHostAddresses() 구현
    !- docker-compose file의 network: 블럭 전에 생기는 빈줄을 제거하자
      . writer1x4Dockernet/writer1x4Hostnet prometheus_port_forward 처리 문제
  !- addch fix
    !- --out 지정하지 않더라도 일은 성공하게
    !- 기존 peer/orderer (신규 org peer 말고) extra_hosts 처리?
      => 기존 orderer/peer들이 변경 없는 상태에서 ch만 추가하므로, extra_hosts 처리 불필요
  !- addpeer fix
    !- --out 지정하지 않더라도 일은 성공하게
    !- addpeer시 모든 peer들이 서로 연결된다. addorg는 그렇지 않았다. 뭐가 다른가?
      => addpeer 의 경우, 기존 orderer/peer 주소를
        addpeer-master.sh 이 신규 peer 컨테이너의 /etc/hosts 에 추가한다.
      !- addpeer-master.sh 에서 hosts 파일 핸들링을 제거하고,
        peer3.org1 docker-compose 파일에 extra_hosts를 추가해서 HLF nw 이 구성되는지 확인하자
        => 안된다. addpeer-master.sh의 hosts 핸들링이 범인 맞다.
    !- docker-compose,run-cli 에 기존 peer/orderer extra_host 추가
    !- addpeer-master.sh 간략화: hosts 파일 핸들링 제거
  !- addorg, ip-addr 기준으로 docker-compose 작성하는 것이 옳을지 다시 생각
    . 새로추가되는 org & host를 모두 검사해서 docker-compose를 작성할 수도 있다
    => 신규 host ip를 기준으로 docker-compose를 작성하는 것이 적절하다고 판단
    => 기존 peer/orderer 컨테이너가 존재하는 host ip 일 경우에도,
      docker-compose up 명령을 중복 실행시켜도
      기존 peer/orderer 컨테이너가 중복 실행되지 않으므로 안전
  !- README fix
    . current-hlf.yaml의 ip addr을 수정하고 시작해야 한다.
    . --generate-only 옵션을 지정하면 컨테이너를 자동 생성하지 않는다
      (step-by-step 실행이 가능하다).
    . "--generate-only 로 파일 생성 -> add[org|ch|peer]-master.sh 등 검토/수정 -> 실행" 추천
  !- full test
0120- BaaS 관련 라이트닝 디비 (노홍찬PL님) 협의
  !- 한 일? 절차? 소요 비용?
  => AWS Marketplace에 AMI 등록
  => 베스핀의 도움을 받음 (절차 200만원 + 1년 유지보수 = 3~400만원),
    BP 도움을 받지 않는다면 비용 없이도 가능
  => AMI 스터디 및 준비에 2달, AWS 등록/승인 프로세스에 1달 정도 소요
  => 고객 대응 프로세스는 크게 고민하지 않음 (AMI 상품이라 가능)
0120- BaaS Sale 자료 작성
  - 해결하고자 하는 pain-point : HLF 학습 기간 단축
  - SKT가 제공할 수 있는 value :
    - provisioning tool, admin tool
    - SKT HLF asset (api, token-cc, monitoring-page, ...)
    - 개발된 것, 개발할 것 정리
